---
title: My first Blogpost
author: Simon Buettner
date: '2019-09-01'
slug: my-first-blogpost
categories: []
tags: []
keywords:
  - tech
---



<div id="beginning-a-blog" class="section level1">
<h1>Beginning a Blog</h1>
<p>In my first blog post I want to investigate a data set that I found on <a href="https://openpsychometrics.org/_rawdata/" class="uri">https://openpsychometrics.org/_rawdata/</a>, namely the “Protestant Work Ethic Scale” consisting of 19 likert scale items with n = 558 answers. Ever since I was interested in predictors of job success, where in numerous studies it shown that following measures of General Intelligence, Conscientiousness (and/or Integrity) was the leading predictor of job success, I thought “Wow, so old fashioned virtues really do hold today (overall) when it comes to being succesful. Why wouldn’t someone build a scale to measure just those virtues?” More explicitly, since in Germany we know the classic German virtues (orderliness, being punctual, working hard etc.) which are basically derived from Prussian virtues and as of today are still THE virtues people in Suevia where I was “born and raised” and still live today, I found some amusement in thinking of a “German/Prussian/Suevian virtues scale” (in german a “schwäbische Hausfrauenskala” would reflect this in a more humorous manner). Fast forward to last friday when I wanted to have a peek at freely available datasets that I could investigate with some models available with current R psychometric packages, I find this little gem that goes back to Max Webers “Die protestantische Ethik und der Geist des Kapitalismus” where he basically posits that the rather ascetic virtues of protestants in northern europe played a key role in the development of (early) capitalism. Naturally I was excited to investigate this data. Unfortunately, there is no possibility to investigate whether this scale predicts job success, nonetheless the underlying structure of this scale might reveal some great insights (and to be honest fitting some models is fun in and of itself). Let’s do it then.</p>
<p>But first, what does this scale even look like? Here are the items:</p>
<ul>
<li>Q1 Most people spend too much time in unprofitable amusements.</li>
<li>Q2 Our society would have fewer problems if people had less leisure time.</li>
<li>Q3 Money acquired easily (e.g. through gambling or speculation) is usually spent unwisely.</li>
<li>Q4 There are few satisfactions equal to the realization that one has done one’s best at a job.</li>
<li>Q5 The most difficult college courses usually turn out to be the most rewarding.</li>
<li>Q6 Most people who don’t succeed in life are just plain lazy.</li>
<li>Q7 The self-made person is likely to be more ethical than someone who is born to wealth.<br />
</li>
<li>Q8 I often feel I would be more successful if I sacrificed certain pleasures.</li>
<li>Q9 People should have more leisure time to spend in relaxation.<br />
</li>
<li>Q10 Anyone who is able and willing to work hard has a good chance of succeeding.</li>
<li>Q11 People who fail at a job have usually not tried hard enough.</li>
<li>Q12 Life would have very little meaning if we never had to suffer.</li>
<li>Q13 Hard work offers little guarantee of success.</li>
<li>Q14 The credit card is a ticket to careless spending.</li>
<li>Q15 Life would be more meaningful if we had more leisure time.</li>
<li>Q16 The person who can approach an unpleasant task with enthusiasm is the one who gets ahead.</li>
<li>Q17 If one works hard enough they are likely to make a good life for themselves.</li>
<li>Q18 I feel uneasy when there is little work for me to do.</li>
<li>Q19 A distaste for hard work usually reflects a weakness of character.</li>
</ul>
<p>As you can see, this is scale kind of represents your (grand-)parents waving their finger and telling you not to waste your time, to save some money, and that you should work hard. Before taking a first look at the underlying data structure, I will take a away one result: Q7 and 14 kind of don’t work in any factorial structure and just add some noise to the data. So, for the further analyses, I excluded them.</p>
<p>This already tells us a little something: credit card spending was not really seen as being associated with the rather a rather ascetic ethic. I’d guess this item was kind of meant to assess the way people feel about taking credit in order to buy something instead of waiting until they have saved enough money to buy it (in general I believe this idea would not work too well in every country. In the U.S. this might be rather acceptable while in germany the only thing you’d buy on debt is a house - at least in my social surrounding). But the items failed to reflect this mindset, and the data shows it.</p>
<p>Item Q7 kind of targets the believe that rich people are somehow decadent or at least more decadent than a self-made man (and perhaps even don’t deserve their wealth). I can see why this does not really fit the rest of the scale because it targets a rather specific oppinion, that does not really need to be in line with the general ethical principles glimmering through the rest of these items.</p>
</div>
<div id="investigating-the-factor-structure-fa.parallel" class="section level1">
<h1>Investigating the Factor Structure: fa.parallel()</h1>
<p>Enough talk, let’s get down to business. To investigate the underlying data structure, the function fa.parallel() really is the way to go. It creates a Factor Analysis and a Principal Component Analysis Scree Plot. And not only that, it also simulates scree plots with randomly filled data matrices identical in structure to your data. This way, you can see how much of the structure you find is just noise and how much of it should be considered signal, i.e. nonrandom “real” structure. Since I am not really familiar with the FA Scree Plot, I usually just look at the PCA one mainly but try to use the FA plot in a rather informal way. In our case, we clearly see that there is one big component that we should extract and one additional factor less than 3 times as important. The FA Scree Plot would suggest that there are up to 6 more factors (but again way less important than the big first factor) to be considered, though I guess only 4 of them would be reasonable to have a look at. So, whatever we are going to model, we will need one big factor for sure and up to 4 smaller factors.</p>
<pre class="r"><code>fa.parallel(dat)</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre><code>## Parallel analysis suggests that the number of factors =  5  and the number of components =  1</code></pre>
</div>
<div id="what-factors-are-there-omega" class="section level1">
<h1>What factors are there? omega()</h1>
<p>Next we want to see how items actually go together. The easiest way to investigate this (imo), is to just use the plot that omega() gives us.
Again I’m kind of taking away some of the upcoming results, but overall, the extraction of a 4 factor structure will lead to some good looking und easy to use results. Thus, we will only look at the omega plot for 4 factors. Since omega will use a bifactor model, this means we will get 1 general factor and 3 narrower factors - exactly what our fa.parallel result would urge us to do. I’m not going to interpret these results in detail, because I really just use the omega plot for some preliminary data inspection. The reason for this is rather simple: the omega plot gives us a bifactor structure, but I don’t really know if I want to trust this one, because it allows crossloadings and uses a classical linear metric approach to get it’s solutions while later on I will prefer item response theory approaches that actually model the association between latent construct und response behaviour. In my experience, the omega bifactor structure does not converge perfectly with the in depth analysis that IRT modelling allows us to do.</p>
<pre class="r"><code>omega(dat,3)</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre><code>## Omega 
## Call: omega(m = dat, nfactors = 3)
## Alpha:                 0.89 
## G.6:                   0.9 
## Omega Hierarchical:    0.74 
## Omega H asymptotic:    0.82 
## Omega Total            0.9 
## 
## Schmid Leiman Factor loadings greater than  0.2 
##          g   F1*   F2*   F3*   h2   u2   p2
## Q1A   0.56  0.21             0.39 0.61 0.80
## Q2A   0.51              0.38 0.41 0.59 0.63
## Q3A   0.41  0.25             0.24 0.76 0.73
## Q4A   0.53  0.33             0.39 0.61 0.72
## Q5A   0.38  0.31             0.25 0.75 0.59
## Q6A   0.57        0.22       0.41 0.59 0.80
## Q8A   0.38                   0.19 0.81 0.79
## Q9A-  0.55              0.58 0.64 0.36 0.48
## Q10A  0.63        0.54       0.70 0.30 0.58
## Q11A  0.58  0.24             0.42 0.58 0.80
## Q12A  0.43  0.25             0.28 0.72 0.67
## Q13A- 0.52        0.38       0.42 0.58 0.63
## Q15A- 0.53              0.52 0.55 0.45 0.50
## Q16A  0.46        0.20       0.30 0.70 0.72
## Q17A  0.69        0.47       0.71 0.29 0.68
## Q18A  0.37  0.29             0.23 0.77 0.60
## Q19A  0.63  0.34             0.51 0.49 0.77
## 
## With eigenvalues of:
##    g  F1*  F2*  F3* 
## 4.64 0.72 0.81 0.85 
## 
## general/max  5.43   max/min =   1.18
## mean percent general =  0.68    with sd =  0.1 and cv of  0.15 
## Explained Common Variance of the general factor =  0.66 
## 
## The degrees of freedom are 88  and the fit is  0.49 
## The number of observations was  548  with Chi Square =  263.65  with prob &lt;  2e-19
## The root mean square of the residuals is  0.04 
## The df corrected root mean square of the residuals is  0.05
## RMSEA index =  0.061  and the 10 % confidence intervals are  0.052 0.069
## BIC =  -291.3
## 
## Compare this with the adequacy of just a general factor and no group factors
## The degrees of freedom for just the general factor are 119  and the fit is  1.33 
## The number of observations was  548  with Chi Square =  716.64  with prob &lt;  4.3e-86
## The root mean square of the residuals is  0.08 
## The df corrected root mean square of the residuals is  0.09 
## 
## RMSEA index =  0.097  and the 10 % confidence intervals are  0.089 0.103
## BIC =  -33.8 
## 
## Measures of factor score adequacy             
##                                                  g   F1*   F2*  F3*
## Correlation of scores with factors            0.87  0.59  0.69 0.73
## Multiple R square of scores with factors      0.76  0.35  0.48 0.54
## Minimum correlation of factor score estimates 0.51 -0.31 -0.04 0.07
## 
##  Total, General and Subset omega for each subset
##                                                  g  F1*  F2*  F3*
## Omega total for total scores and subscales    0.90 0.79 0.83 0.77
## Omega general for total scores and subscales  0.74 0.60 0.59 0.41
## Omega group for total scores and subscales    0.11 0.19 0.23 0.36</code></pre>
</div>
<div id="the-graded-response-model-model-fit" class="section level1">
<h1>The graded response model: model fit</h1>
<p>Being an IRT fanboy, I usually test various modelling approaches to see how to best model my data. I recently started using lavaan more often, mainly because it allows for faster model estimation and because it can propose relations to be modelled. But since what I am really interested in can only be investigated through some IRT modelling, lavaan is just my henchman for the following “real” analysis. For this analysis, I rely on the “mirt” package from R. Philip Chalmers. It is the most flexible IRT modelling package so far, and if you have some time to dive into one package, mirt is really the go to package. I startet this paragraph by outing my self as an IRT fanboy, but I’m even a bigger mirt fanboy and would probably not be the former if it wasn’t for the latter.</p>
<p>The first thing we will do is just fit a unidimensional model, because to be honest, the Scree Plot analysis really could just make me ignore any more structure than the one big factor. I’ve included the way I would estimate the mirt models, but some of the modelling just takes up too much time, so I commented these strips out.</p>
<pre class="r"><code># gmodel1 &lt;- mirt(dat, 1, &quot;graded&quot;, TOL = .0001)
summary(gmodel1)
##          F1    h2
## Q1A   0.655 0.429
## Q2A   0.679 0.461
## Q3A   0.490 0.240
## Q4A   0.628 0.394
## Q5A   0.457 0.209
## Q6A   0.749 0.561
## Q8A   0.459 0.211
## Q9A  -0.691 0.478
## Q10A  0.766 0.587
## Q11A  0.693 0.481
## Q12A  0.522 0.273
## Q13A -0.662 0.438
## Q15A -0.673 0.453
## Q16A  0.596 0.355
## Q17A  0.825 0.681
## Q18A  0.440 0.193
## Q19A  0.733 0.537
## 
## SS loadings:  6.98 
## Proportion Var:  0.411 
## 
## Factor correlations: 
## 
##    F1
## F1  1
# (m2.gmodel1&lt;-M2(gmodel1, type = &quot;C2&quot;))
m2.gmodel1
##             M2  df p      RMSEA   RMSEA_5   RMSEA_95      SRMSR       TLI
## stats 478.7592 119 0 0.07434279 0.0673893 0.08129151 0.06275646 0.8971765
##             CFI
## stats 0.9100295
itemfit(gmodel1, method = &quot;PV_Q1&quot;)
##    item    S_X2 df.S_X2 p.S_X2
## 1   Q1A  90.209      94  0.592
## 2   Q2A  96.179      76  0.059
## 3   Q3A 105.028     104  0.453
## 4   Q4A  84.754      93  0.717
## 5   Q5A 112.194     116  0.583
## 6   Q6A  63.159      77  0.872
## 7   Q8A  93.221     102  0.721
## 8   Q9A  85.313      85  0.470
## 9  Q10A 106.672      79  0.021
## 10 Q11A  55.734      89  0.998
## 11 Q12A  82.253     105  0.951
## 12 Q13A 100.341      96  0.361
## 13 Q15A 118.786     103  0.137
## 14 Q16A  87.002      86  0.449
## 15 Q17A  96.273      74  0.042
## 16 Q18A  96.974     111  0.826
## 17 Q19A  85.034      85  0.479</code></pre>
<p>The summary shows us two things: The first factor explains a good chunk of the variance in the data (41%) and all items load decently on this general factor.
“M2” gives us some CFA indices and we can see, the one dimensional graded response model fits rather decently! However, I must remind you, we already excluded 2 items at this point. Additionally, there is still some room for improvements in every CFA index.
Another thing we straight up want to look at is the model conformity of single items. Here too, in terms of significance, we do not find heavily misfitting items.</p>
<p>In order to see what that means, we can have a quick look at the empirical plot of the misfitting and a fitting item. And what we see is … well not to much. There are some data points that don’t really the expected pattern, but we do not see anything extreme. I think most disturbing is that the curve for Category 1 is really steep towards the lower where there are not a lot of data points.</p>
<pre class="r"><code>itemfit(gmodel1, empirical.plot = 9, group.bins = 16)</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>itemfit(gmodel1, empirical.plot = 16, group.bins = 16)</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
</div>
<div id="the-graded-response-model-person-parameters" class="section level1">
<h1>The graded response model: person parameters</h1>
<p>Next we want to make some analysis to go in direction of applicability of the scale. Therefor we have a look at the reliability of the 1 dimensional GRM and the theta (person parameter) distribution.
The theta distribution is really disturbing. To be honest, the distribution very much seems to give us and our modelling approach the finger. Yikes! How would we even use this in a real life setting? Normal distribution not in sight.</p>
<pre class="r"><code># thetgm1 &lt;- fscores(gmodel1, full.scores.SE = TRUE, method = &quot;EAP&quot;)

hist(thetgm1, breaks = 40)</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Oh well, who cares really? Let’s rather turn to the reliability of the scale. This time we’re not disappointed: It’s high throughout the expectable theta range! This does not help us in respect of middle finger peak of the theta distribution (we cannot differentiate a big part of the persons because they are so heavily clustered).
At this point you might be wondering two things: 1. Wait, why is the reliability dependend on theta? 2. Wait, why is this guy coming back to reliability despite the IRT framework? Glad you asked!</p>
<ol style="list-style-type: decimal">
<li><p>In item response theory there is no “natural” single value to reflect how precise our test measures the underlying ability. Instead, every item has the capacity to inform us about the underlying latent trait of a single person. This capacity is an item specific attribute and is called “item information”. The item information is a function of the theta parameter, i.e., some items are decent in order to differentiate between persons on the higher end of the theta spectrum, some in the middle (my experience is: mostly in the middle) and some in the lower end. The easiest way to grasp this is to think of some math test. In this test you ask participants “2+2= ?”. Clearly, this item won’t help us differentiate between anyone beyond middleschool because every one will get it right. However, to test low ability persons (kindergarden kids that just started learning about calculating) this item might just be perfect. Asking for the solution of some complex equation containing some weird integrals however, might help differentiate bteween test persons after middleschool, but asking some kindergarten kids to give you an answer won’t help differentiation between these kids (no matter how many sweets you promise for a correct answer).
So much about item information. The nice thing in IRT is that in order to know how precise a test is overall, you just have to add the single iteminformation functions. The result is the test information function that is depicted in terms of the “reliability” as used in classical test theory.</p></li>
<li><p>Since only a few “normal” psychologists know a big deal about IRT, the use of “reliability” makes things a lot more comprehensible. Of course, this reliability depends perfectly on the test information (and thus on the Standard Error of the theta).</p></li>
</ol>
<pre class="r"><code>empirical_rxx(thetgm1)
##        F1 
## 0.9083824
plot(gmodel1, type = &quot;rxx&quot;, theta_lim = c(-2,2) )</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Since we get the fat clustering of theta values, we’re gonna be interested in the model more in general. We will therefor look at the expected score given our model, at the expected chosen category of the model and the expected item score - all depending on theta. We do not really find oddities here. Except that (plot 2) item response behavior is not in line with the usual likert scale assumption of equal intervals between item scores.</p>
<pre class="r"><code>plot(gmodel1, type = &quot;score&quot;, theta_lim = c(-2,2))</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>plot(gmodel1, type = &quot;trace&quot;, theta_lim = c(-2,2))</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code>plot(gmodel1, type = &quot;itemscore&quot;, theta_lim = c(-2,2))</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
</div>
<div id="the-graded-response-model-short-summary" class="section level1">
<h1>The graded response model: short summary</h1>
<p>So far we’re kind of happy, because our model- and itemfit is decent and we get a great reliability. Yay! However, the theta we get is, imho, just trash. What to do about that? We need some different modelling approach, and, lucky as we are, a different modelling approach can be used righteously - just remember our extra small factors!</p>
</div>
<div id="bifactor-models-of-protestant-ethics." class="section level1">
<h1>Bifactor models of protestant ethics.</h1>
<div id="specific-factors" class="section level2">
<h2>2 Specific factors</h2>
<pre class="r"><code>spec1 &lt;- rep(NA, 18)
spec1[c(10,16,13)] &lt;- 1
spec1[c(9,14,2)] &lt;- 2
# bf1 &lt;- bfactor(dat, spec1, TOL = .0001)
summary(bf1)
##           G     S1     S2     h2
## Q1A   0.668  0.000  0.000 0.4461
## Q2A   0.640  0.000 -0.368 0.5452
## Q3A   0.516  0.000  0.000 0.2667
## Q4A   0.648  0.000  0.000 0.4198
## Q5A   0.497  0.000  0.000 0.2471
## Q6A   0.751  0.000  0.000 0.5642
## Q7A   0.304  0.000  0.000 0.0927
## Q8A   0.494  0.000  0.000 0.2439
## Q9A  -0.622  0.000  0.654 0.8145
## Q10A  0.693  0.571  0.000 0.8061
## Q11A  0.724  0.000  0.000 0.5248
## Q12A  0.544  0.000  0.000 0.2956
## Q13A -0.594 -0.390  0.000 0.5052
## Q15A -0.599  0.000  0.614 0.7353
## Q16A  0.601  0.000  0.000 0.3607
## Q17A  0.773  0.477  0.000 0.8254
## Q18A  0.463  0.000  0.000 0.2140
## Q19A  0.765  0.000  0.000 0.5849
## 
## SS loadings:  6.845 0.706 0.941 
## Proportion Var:  0.38 0.039 0.052 
## 
## Factor correlations: 
## 
##    G S1 S2
## G  1  0  0
## S1 0  1  0
## S2 0  0  1</code></pre>
<pre class="r"><code>theta_bf1 &lt;- fscores(bf1, full.scores.SE = TRUE)
hist(theta_bf1[,&quot;F1&quot;])</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>hist(theta_bf1[,&quot;F2&quot;])</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<pre class="r"><code>hist(theta_bf1[,&quot;F3&quot;])</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-9-3.png" width="672" /></p>
<pre class="r"><code>empirical_rxx(theta_bf1)
##        F1        F2        F3 
## 0.8940109 0.5484565 0.6066023</code></pre>
</div>
<div id="specific-factors-1" class="section level2">
<h2>3 Specific factors</h2>
<pre class="r"><code>spec2 &lt;- rep(NA, 18)
spec2[c(6,4,17)] &lt;- 1
spec2[c(10,16,13)] &lt;- 2
spec2[c(9,14,2)] &lt;- 3
# bf2 &lt;- bfactor(dat, spec2, TOL = .001)
summary(bf2)
##           G     S1     S2     S3     h2
## Q1A   0.665  0.000  0.000  0.000 0.4422
## Q2A   0.644  0.000  0.000 -0.365 0.5482
## Q3A   0.510  0.000  0.000  0.000 0.2598
## Q4A   0.656  0.442  0.000  0.000 0.6259
## Q5A   0.495  0.000  0.000  0.000 0.2454
## Q6A   0.777 -0.282  0.000  0.000 0.6835
## Q7A   0.302  0.000  0.000  0.000 0.0913
## Q8A   0.489  0.000  0.000  0.000 0.2395
## Q9A  -0.620  0.000  0.000  0.654 0.8118
## Q10A  0.695  0.000  0.568  0.000 0.8048
## Q11A  0.728  0.000  0.000  0.000 0.5306
## Q12A  0.537  0.000  0.000  0.000 0.2881
## Q13A -0.594  0.000 -0.390  0.000 0.5048
## Q15A -0.596  0.000  0.000  0.617 0.7363
## Q16A  0.592  0.000  0.000  0.000 0.3506
## Q17A  0.774  0.000  0.476  0.000 0.8259
## Q18A  0.462  0.428  0.000  0.000 0.3961
## Q19A  0.760  0.000  0.000  0.000 0.5775
## 
## SS loadings:  6.861 0.458 0.7 0.942 
## Proportion Var:  0.381 0.025 0.039 0.052 
## 
## Factor correlations: 
## 
##    G S1 S2 S3
## G  1  0  0  0
## S1 0  1  0  0
## S2 0  0  1  0
## S3 0  0  0  1</code></pre>
<pre class="r"><code>theta_bf2 &lt;- fscores(bf2, full.scores.SE = TRUE)
## Warning: High-dimensional models factor scores should use quasi-Monte Carlo
## integration. Pass QMC=TRUE
hist(theta_bf2[,&quot;F1&quot;], breaks = 18)</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>hist(theta_bf2[,&quot;F2&quot;])</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code>hist(theta_bf2[,&quot;F3&quot;])</code></pre>
<p><img src="/post/2019-09-01-my-first-blogpost_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
<pre class="r"><code>empirical_rxx(theta_bf2)
##        F1        F2        F3        F4 
## 0.8942307 0.4207336 0.5917202 0.6328800</code></pre>
</div>
</div>
