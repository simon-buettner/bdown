---
title: Effektstärken 2
author: ''
date: '2020-11-22'
slug: effektstärken-2.en-us
categories: []
tags: []
keywords:
  - tech
---
```{r setup, include=FALSE}
library(effectsize)
```

# Psychologische Befunde ...  aber was bedeuten sie?
Nachdem wir im letzten Blogpost ein wenig das Geheimnis gelüftet haben, was Effektstärken statistisch betrachtet eigentlich bedeuten, fehlt noch immer die Interpretation im Sinne einer Einordnung der Effekte. 
Dem widmen wir uns jetzt. 
Den Artikel von Funder und Ozer (2019) habe ich beim letzten Eintrag bereits erwähnt, dort werden in kurzer und interessant geschriebener Form die meisten Punkte behandelt, wer also keine Scheu vor wissenschaftlichen Artikeln hat, dem sei dieser nochmal ausdrücklich empfohlen. 
Die Punkte aus dem Artikel werde ich an einigen Stellen verkürzt, an anderen erweitert, hier vorstellen.

Bevor wir richtig einsteigen, möchte ich die Links zu einigen der unten referierten Studien anfügen. 
In diesen Studien finden sich mehrere lange Tabellen zu wissenschaftlichen (nicht nur psychologischen) Ergebnissen mit assoziierten Effektstärken -- für jeden, der seinen Corona-Stubenarrest produktiv nutzen möchte, indem er diese einmal durchschmökert.

Viele psychologische, aber auch medizinische und allgemeinere Effekte: [Meyer et al. (2001): Psychological Testing and Psychological Assessment: A Review of Evidence and Issues.](https://d1wqtxts1xzle7.cloudfront.net/40609242/Psychological_testing_and_psychological_20151203-10914-1w1gy5m.pdf?1449159412=&response-content-disposition=inline%3B+filename%3DPsychological_testing_and_psychological.pdf&Expires=1605348030&Signature=gfgavvhYbHIIjhEgZfoUengttIteuDPhf4o3HDtVQjEL~g8kztSDwIJktcgB4eIUihniJaQsA0RHaR4Pvq2~UZrJ5RWk9mpGvDrL4ePwHplXneedmTJGspKf7FH7eLrhdCNndOv~f6209mCdvZ9uh7nOl55b2rVO9NQL7b2O2ag45f8g1cs2lG28t0HtnkOZni-Jhsvv2DNzQ0sZH2B7amoGJxza9tM5rqE3Gm~6d8b97iZs~tWFdrRjxJA-iTA9x4KvwhATd8ht-tVqfkrDbCz46Uu0~4SI0BHNMCLEn~FltUiGcB80tnMnBHX1bPrKIrHf5v~heTF24pz9D31fDA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)

Viele sozialpsychologische Effekte im Anhang: [Richard, Bond und Stokes-Zoota (2003): One Hundred Years of Social Psychology Quantitatively Described.](https://d1wqtxts1xzle7.cloudfront.net/50574607/One_Hundred_Years_of_Social_Psychology_Q20161127-6595-oief2k.pdf?1480266886=&response-content-disposition=inline%3B+filename%3DOne_Hundred_Years_of_Social_Psychology_Q.pdf&Expires=1605348421&Signature=OY1-Etyk6YYyJZ1cVdp7ip-g~iAawAdLJguLDLqFTieWoJ-EDFYi1PyLdpl2D~U017Oh3PYULzZoL4zCX00pTizkpoS0wuiyROky03AieojLmgPCXr8~Ls9uCoH9EZcI-zYkQxJ5nr1pUqrNgnHjqtbg0GCRNCobUqM8Q2f~AQWQOdn6QIyxaB3W3keh~WM3gphDLBP~2-NIovllZM0dw4T3Yb55z7FcIBYIxHZRoKq66j7fo~Gl8HksvdextoOxHelx2S5zNU1vXeU9ckeuspQToD2ggq884VFWlXwEm6LHH~FkeW8jRnPrgM~A1WYe0VjVw4bUvNjDie1WhwnBgw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)


# Benchmarks

Die einfachste Art, Effekte und ihre Größe einzuschätzen, ist der Vergleich mit anderen Effekten. 
Das ist so einfach, dass schon mehrere Autoren verschiedenste Effekte zusammengetragen haben, auf die wir dankbarerweise zurückgreifen können, um zu entscheiden: das ist ein vergleichsweise großer Effekt, jener ist ganz typisch und dieser ist vergleichsweise klein. 
Nun sind die meisten meiner Leser wohl weniger tief in Forschungsarbeiten vertieft (noch weniger berechnen jemals selbst irgendeinen Effekt), weshalb das „Benchmarking“ der Effekte zu einem netten Nebeneffekt führt, denn so lässt sich einmal in Erfahrung bringen, wie groß bestimmte wissenschaftliche Effekte denn grundsätzlich ausfallen. 

Am eindrücklichsten ist das meiner Meinung nach bei medizinischen Effekten, von denen jeder einige wie Sprichwörter auf der Zunge trägt: „Aspirin verhindert Herzanfälle“, „Kurkuma beugt Krebs vor“ oder „die grünen Stiele von Tomaten verursachen Krebs – weg damit“.
Das sind die für unsere Zeit typischen „Volksweisheiten“, jeder schnappt sie in irgendwelchen Sendungen auf oder liest sie hie und da (manchmal mit Zitat), aber die wenigsten wissen, wie hilfreich oder wie gefährlich Stoff X für die eigene Gesundheit ist oder wie paranoid man mit den grünen Tomatenstielen umgehen sollte - was also das wissenschaftliche Fundament hinter den Weisheiten eigentlich ist. 
Das ist nicht böse gemeint – auch ich habe eine Vorliebe solche Weisheiten und halte mich selbst an einige, die einer rationalen Betrachtung kaum standhalten dürften (wo ich stehe, das verdanke ich der heilsamen Kraft des Leinöls). 
Umso schöner, dass man hier den eigenen Horizont erweitern kann und am Ende des Artikels das Rüstzeug erhalten haben wird, eigenständig einige Medizinpaper zu durchforsten, um die (relative) Gefahr grüner Tomatenstiele im Gesamtbild zu betrachten (wohl gemerkt, ich werde das nicht machen).

Bevor wir zu den Effekten selbst kommen, möchte ich die einfachsten bereits seit 30 Jahren vorhandenen Benchmarks wiedergeben. 
Der Namensgeber des berühmten *d*, **Cohen** (1988), hat seine **Effektstärkeneinteilung** im Grunde auch darauf bezogen, wie bestimmte andere Effekte ausfallen bzw. sie sich sichtbar machen. 
Generell war schon sein Credo: Effektstärken müssen immer im Kontext des Forschungsgegenstands interpretiert werden. Für den Fall, dass das nicht möglich sein sollte, hat er einige allgemeine, zugegebenermaßen teils eigenartige, aber dafür sehr im wahrsten Sinne des Wortes anschauliche Benchmarks angegeben. 
Als solche setzt er die mittleren Körpergrößenunterschiede zwischen verschiedenen Altersstufen von Mädchen (wohl gemerkt: das sind nicht die einzigen Benchmarks, die er zitiert, aber wir begnügen uns mit diesen):

- **kleine Effekte**, *d* zwischen 0.2 und 0.5 (r zwischen ~0.1 und ~0.24): Größenunterschiede zwischen 15 und 16 jährigen Mädchen.
- **mittlere Effekte**, *d* zwischen 0.5 und 0.8 (r zwischen ~0.24 und ~0.37): Größenunterschiede zwischen 14 und 18 jährigen Mädchen. Laut Cohen sind diese „visible to the naked eye“, weshalb ähnlich große Effekte verdienen, von uns „mittelstark“ genannt zu werden.
- **große Effekte**, *d* größer 0.8 (r größer ~0.37): Größenunterschiede zwischen 13 und 18 jährigen Mädchen.

Cohen hatte aber wohl keineswegs beabsichtigt, dass seine grobe Einteilung, die eigentlich auch nur gelten soll, wenn andere Informationen zur Einordnung der Effektstärken nicht vorliegen, derart zombiehaft zitiert wird, wie es Psychologen seit den 90ern praktizieren (Stand 15.11.2020: 186.088 Zitierungen auf Google Scholar. Übrigens, selbstverständlich habe ich die Einteilung auch schon zombiehaft zitiert.). 
Interessant fand ich die Anekdote im Funder und Ozer Artikel, dass Cohen im Nachhinein wohl auch bereut haben soll, diese Einteilung vorgenommen zu haben, denn so wie sie dann genutzt wurde, war sie nicht gedacht. 

Auch wenn die Einteilung meiner Meinung nach vor dem Hintergrund der Sichtbarkeit von Effekten im Alltag (dass 16 jährige Mädchen nicht so viel größer sind als 15 jährige, der Unterschied zwischen 13 und 18 jährig hingegen schon recht eindeutig ist) durchaus sinnig ist, würde ich wie Cohen und viele nach ihm doch auch behaupten, **besser Effekte aus vergleichbaren Forschungsdisziplinen zu nutzen**, um zu entscheiden, wie der spezifische Effekt, den man gefunden hat, einzuschätzen ist. 
Aber das ist halt aufwendig. 
Dass es dennoch sinnvoll ist, wird sich im Weiteren auch zeigen, nicht zuletzt weil zumindest wir Psychologen zumeist etwas betrübt feststellen (müssen), dass wir wieder nur einen relativ unsichtbaren 15-vs.-16-Jahrgrößenunterschiedeffekt gefunden haben. 
Das ist auf Dauer ein bisschen frustrierend, vor allem aber nahezu ohne Informationsgehalt (überspitzt: wenn alle Effekte "klein" zu nennen sind, welche sind dann wirklich klein?).

## Meyer und Kollegen (2001): für eine Handvoll Effektstärken

Bevor wir aber zu spezifisch psychologischen Effekten kommen, gehen wir auf den Rundumschlag von Meyer und Kollegen (2001) ein, der auch in Funder und Ozer (2019) zitiert wird. 
Junge, Junge, Junge, wer ein bisschen zahlenaffin ist, für den ist dieser Artikel mit seinen **ellenlangen Tabellen eine wahre Fundgrube**. 
Im Artikel werden verdichtet die Effekte aus mehreren hundert Artikeln zusammengetragen, wobei ein Ziel war, psychologische und medizinische Diagnostik zu vergleichen (Ja, Hurra, sie sind ähnlich gut). 
Hierfür wurden zunächst nicht-diagnostikspezifische Effekte gesammelt. 
Das allein sind schon so viele, dass ich hier nur eine kleine Auswahl vornehmen konnte. 
Statt Cohen's *d* wurde der etwas allgemeinere Korrelationskoeffizient angegeben, die Anzahl der Studien (bzw. auch Personen in den Studien) spare ich aus, sondern gebe hier einfach den Effekt und das dazugehörige *r* an (man erinnere sich, *d* und *r* lassen sich ineinander umrechnen; die umgerechneten Werte habe ich in der **Grafik** unten dargestellt):

- ***r* = 0.00**: Effekt von Zucker auf das Verhalten und die kognitiven Prozesse von Kindern (gleich ein Hammer für alle Eltern, die das vermeintliche Zuckerhigh ihrer Kinder scheuen).
- ***r* = 0.02**: die Reduktion des Herzinfarktrisikos durch Aspirin. 
- ***r* = 0.11**: Einnahme von Antihistaminen und damit zusammenhängende Reduktion von Niesen und einer laufenden Nase.
- ***r* = 0.14**: Schmerzverringerung nach Einnahme von nichtsteroidalen entzündungshemmenden Medikamenten, wie z.B. Ibuprofen (wobei ich mir kaum vorstellen kann, dass der Effekt bei Kopfschmerzen so gering ist, aber das wird bei Meyer et al. nicht aufgedröselt und ich schau's nicht nach).
- ***r* = 0.21**: Zusammenhang zwischen [sozialer Unterstützung](https://en.wikipedia.org/wiki/Social_support) und gesteigerter Immunfunktion.
- ***r* = 0.32**: Effekt einer klinischen Depression auf verringerte Immunfunktion.
- ***r* = 0.38**: Effekt von Viagra auf … na! Wenn man in Zukunft irgendwo von Effekten von r = 0.38 liest oder sie findet, kann man also wunderbar davon sprechen, dass beispielsweise „Therapie X das Viagra unter den Therapien ist“. 
- ***r* = 0.44**: Zusammenhang zwischen Größe und Gewicht bei US-Erwachsenen.
- ***r* = 0.52**: Abnahme der Informationsverarbeitungsgeschwindigkeit mit steigendem Alter.
- ***r* = 0.60**: Nähe zum Äquator und Tagestemperatur (USA).
- ***r* = 0.67**: Geschlechterunterschiede der Körpergröße bei US-Erwachsenen.

Das nur ein kleiner Ausschnitt und die Tabellen zu psychologischen und medizinischen Diagnostik sind noch viel länger! 
Insgesamt fällt auf – und da ist meine Aufzählung etwas irreführend – es gibt viel mehr Effekte unter .3 als darüber. 
Diese erste wichtige Feststellung wird sich im Weiteren bestätigen. 

```{r}
r <- seq(0,.7,.01)
d <- r_to_d(seq(0,.7,.01))
plot(r, d, main = "Pearson's r und entsprechendes Cohen's d", type="l", xaxt = "n", yaxt= "n")
axis(1, at = seq(0, .7, by = .05), las=2)
axis(2, at = seq(0, 3, by = .1), las=2)
points( x = .2, y = r_to_d(.2) )
text( x = .2+.22, y = r_to_d(.2),
      labels = "typische Effekte der Psychologie, r ~ 0.2" )
```

## Psychologische Effektstärken: das ganz große Gesamtbild
So weit so gut, wir möchten aber dennoch nicht immer in solchen Tabellen nachschauen, wenn wir einem Effekt aus „Psychologie heute“ genauer nachgehen. 
Außerdem ist uns noch nicht wirklich geholfen, wenn wir irgendeinen medizinischen Effekt mit irgendeinem psychologischen Effekt vergleichen wollen (auch wenn der Vergleich natürlich interessant ist und hilft, einen Effekt im Großen und Ganzen einzuordnen). 
Wir erwarten ja auch nicht bombenfest deterministische Effekte wie in der Physik, wenn wir Verhaltensdaten betrachten. 
Wir müssen schon Äpfel mit Äpfeln vergleichen! 
Doch auch hier müssen wir nicht anfangen mühsam Studien zu durchforsten, um mit dem großen Vergleichen zu beginnen. 

**Richard, Bond und Stokes-Zota (2003) haben das für sozialpsychologische Effekte** und **Gignac und Szorodai (2016) für persönlichkeitspsychologische Effekte** bereits geleistet.
Die beiden Disziplinen unterscheiden sich häufig methodisch dahingehend, dass Sozialpsychologen experimentelle Manipulationen vergleichen (läuft häufig auf Mittelwertunterschiede hinaus) während Persönlichkeitpsychologen häufig untersuchen, wie bestimmte Persönlichkeitseigenschaften mit irgendeinem Kriterium (z.B. Berufserfolg) zusammenhängen.
Deswegen lassen sich beide eigentlich nicht so großartig vergleichen.
Ferner lassen lassen sich experimentelle Untersuchungen untereinander schon schwer vergleichen: Wie soll man ein Experiment, in dem ein minimaler Unterschied zwischen den Experimentalgruppen betrachtet wird (z.B. vor der Klausur wird ein oder kein Apfel gevespert) mit einem, in dem die Experimentalgruppen sich stark unterscheiden (z.B. 4 Wochen lang tägliche 1-stündige Mentalcoachings vs. keine solche Coachings)?
Interessanterweise haben dennoch beide ähnliche Tendenzwerte der Effektstärken gefunden. 
Richard, Bond und Stokes-Zota (2003) haben für die von ihnen untersuchten **sozialpsychologischen Effekte** eine Median-Effektstärke von ***r* = 0.18** gefunden.
Gignac und Szorodai (2016), die sich Korrelationen vornehmlich aus der **Persönlichkeitspsychologie** angeschaut haben, finden als Effektstärke eine Median-Korrelation von ***r* = .19**. 
Das heißt, 50% der Effekte liegen über, 50% liegen unter *r* = .19 (bei den Sozialpsychologen eben über/unter *r* = 0.18). 
Außerdem landen 25% der Ergebnisse höchstens bei *r* = .11, und 75% liegen unter *r* = .29 (für Richard, Bond und Stokes-Zota sind diese Werte nicht angegeben). 

Sehr fein, zumindest für sozial- und persönlichkeitspsychologische Effekte lohnt es sich also, diese groben Benchmarks im Hinterkopf zu behalten.
Jetzt sind wir aber immer noch nicht am Ende der Reise angelangt: 1. wissen wir zwar seit dem letzten Blogpost, wie Gruppenunterschiede statistisch aussehen, nicht aber wie eine Korrelation *r* von .19 aussieht, und 2. wissen wir nicht, wie sich solche Effekte in tatsächlichen Ereignissen widerspiegeln (wie viele Leben werden denn jetzt durch die Herzinfarkt-Prävention mit Aspirin gerettet?). 
Außerdem fragen wir uns noch: **3. kann man das alles überhaupt für bare Münze nehmen?** und **4. wie allgemeingültig sind diese Benchmarks für unterschiedliche psychologische Disziplinen?** 

Die ersten zwei Punkte werden wieder ein eigener Blogpost (und ich dachte doch tatsächlich, dass ich einen schönen kurzen Effektstärken-Blogpost über den Funder und Ozer Artikel schreibe!) mit ShinyApp. 
Der letzte kratzt wieder das Thema **Replizierbarkeit**, wofür ich nur eine Studie etwas genauer vorstellen möchte, die auch 4. etwas aufklärt. 

## Negative Effektdeflation und erfolgreiche Nicht-Replikation

Wenn die meisten meiner Nachbarn Millionäre sind, dann sind diese dennoch nur reich, wenn ich nicht gerade im Zimbabwe von vor einigen Jahren lebe oder ich 1923 nach der Ruhrkrise in Deutschland lebe. 
Ebenso: Wenn die meisten der Psychologen Effekte um *r* = 0.2 finden, dann muss ich schon betrachten, was diese Effekte wert sind (hinkt dieser Vergleich schon genug um Vergleich genannt zu werden?). 
Fragwürdige Forschungspraktiken („Questionable Research Practices“) und „Publication Bias“ (Nullergebnisse werden seltener veröffentlicht und verstauben dann in der Schublade) machen es leider sehr wahrscheinlich, dass wir es mit einer Effektinflation in der veröffentlichten Forschung zu tun haben.

Für den interessierten Laien bedeutet das, sich eingeladen zu fühlen, sich eine gesunde (!) Skepsis zu erhalten, weil man davon ausgehen sollte, dass gefundene Effekte (so sie denn echt sind) etwas geringer ausfallen könnten als veröffentlicht.
Für die Forscherkaste ist das noch etwas wichtiger, denn man möchte beim Vergleich seiner Effekte eigentlich eher wissen, wie groß diese in Wahrheit sind, weniger, wie groß die „frisierten“ Effekte ausfallen – das ist im Zweifel auch sehr selbstwertdienlich und hilft auch bei der Studienplanung (Stichwort Poweranalyse). 
Um den Unterschied zwischen potenziell überschätzten und den „wahren“ Effekten zu untersuchen, haben **Schäfer und Schwarz (2019) traditionell veröffentlichte Effekte mit denen verglichen, die am Ende einer Studie mit durchloffenem Pre-Registration Prozess stehen**. 

Pre-registrierte Studien sollen mehr Transparenz der Forschungspraxis erlauben und könnten dieser etwas frischen wissenschaftlichen Lebenshauch einflößen.
Im Rahmen eines solchen Prozesses meldet man z.B. erst in einem relativ detaillierten Protokoll an, was und wie man es untersuchen möchte. 
Idealerweise wird dann schon die angemeldete Untersuchung akzeptiert, sodass man seine Studie auch dann veröffentlicht, wenn man „nichts“ findet. 
Dann erhebt man seine Daten und wertet alles so aus, wie man es angegeben hatte. 
Da schon vorher klar ist, was gerechnet wird und man auf jeden Fall etwas veröffentlicht, hat man eine geringere Verzerrung in dem, was veröffentlicht wird.
Ferner:

- Es werden nicht im Nachhinein, wenn die Ergebnisse schon feststehen, Hypothesen gebildet, die dann – Oh Wunder – bestätigt werden (ich kann es gerad leider nicht zitieren, aber einige Forscher haben einmal untersucht, wie oft Psychologen ihre Hypothesen in ihren Studien bestätigt fanden. Das war so häufig der Fall, dass man sich beinahe fragen könnte, wieso man die Studien überhaupt noch durchführt, wenn Psychologen doch vorher immer schon wissen, was rauskommen wird). 
- Man schließt nicht irgendwelche Daten aus, weil das zu besseren Ergebnissen führt (das muss nicht in böser Absicht geschehen). 
- Man sammelt nicht wahllos Daten und hofft, dass irgendwelche Ergebnisse statistisch signifikant werden (Schrotflintenmethode). 
- Man schaut nicht ab und an während der Datenerhebung in die Daten und hört auf, zu sammeln, wenn das Ergebnis passt (ja, manche liebäugeln beim Datensammeln mit der Devise „Stop the count!“ und werden dabei nicht einmal vor Scham orange im Gesicht). 
- Und wenn etwas nicht klappt, dann wandert die missglückte Untersuchung nicht in der Schublade des Scheiterns und man beginnt anschließend das Roulettespiel von neuem mit minimalen Anpassungen und hofft, dass es dann ganz bestimmt klappt, denn man hat ja schlaue Sachen angepasst (wieder: hier steckt nicht unbedingt eine böse Absicht hinter der allgemeinen Praxis und sie ist dennoch höchst problematisch!). 

Nein: es wird so viel wie möglich im Voraus festgelegt und wenn halt nichts bei rumkommt, dann wird man trotzdem nicht bestraft, sondern bekommt wenigstens sein Nullergebnis veröffentlicht.

Dass die Kollegen Forschungsergebnisse, die am Ende zweier unterschiedlicher Wege stehen vergleichen, kommt dabei nicht von ungefähr. Kurz erwähnt seien zwei Studien:

- Camerer und Kollegen (2018) untersuchten 21 zwischen 2010 und 2015 in *Nature* und *Science* (sehr prestigeträchtige Journals) veröffentlichte sozialwissenschaftlichte Experimente auf ihre Replizierbarkeit: 
  - Von den mit deutlich größeren Fallzahen replizierten Experimenten wurden in 13 Fällen (= 62%) statistisch signifikante Effekte in die gleiche Richtung wie in den originalen Experimenten gefunden – was als deren Hauptkriterium galt, um von „erfolgreichen Replizierungen“ zu sprechen. 
  - Die Effektstärken nicht-replizierter und replizierter Studien fielen im Schnitt halb so hoch aus wie im Original, mittleres *r* = 0.249 zu mittleres *r* = 0.460. Die Effektstärken der erfolgreich replizierten Studien erreichte ca. 75% der originalen Effektstärken - ein meiner Meinung nach gutes Ergebnis!
  - Außerdem konnte gezeigt werden, dass auf unterschiedliche Weise gesammelte Meinungen dazu, welche Effekte sich wohl replizieren ließen, eine recht gute Vorhersage dafür lieferten, welche Effekte tatsächlich replizierbar waren. Die Forschercommunity scheint also die Replizierbarkeit bzw. Robustheit von Effekten relativ gut einschätzen zu können (aber: da nur 21 Effekte untersucht wurden, muss das natürlich nicht uneingeschränkte gelten).
- Open Science Collaboration (2015), mehrere Forschergruppen untersuchten die Replizierbarkeit von 100 psychologischen Effekten: 
  - Von 100 Effekten konnten „nur“ 39% so repliziert werden, dass diese auch als repliziert eingeschätzt wurden (oder auch andersherum: 61% konnten nicht repliziert werden). 
  - Insgesamt fiel die Effektstärke der replizierten Studien halb so hoch aus wie in den originalen Studien, mittleres *r* = 0.197 zu mittleres *r* = 0.403.
- Es wurden auch weitere größer angelegte Replikationsversuche unternommen, die Ergebnisse sind dabei gemischt und liegen meine ich zwischen der erfolgreicheren Camerer und der weniger erfolgreichen OpenScience Untersuchung. Mal repliziert man mehr, mal weniger. Als versöhnliches Zwischenfazit biete ich an: Es ist nicht alles Gold, es ist nicht alles Mist, aber es lohnt sich weitere Replikationsversuche zu unternehmen!

Das ist erst mal etwas ernüchternd. 
Wer sich gerne mehr mit psychologischer Forschung befassen möchte, für den sei an dieser Stelle ein Rat von Enttäuschten erteilt: Wenn der Effekt groß ist, aber die Stichprobe klein, dann ist das weniger schön, als wenn zwar der Effekt klein, aber die Stichprobe groß ist (… und der Effekt statistisch „hochsignifikant“ ist). 
Hat den schönen Nebeneffekt, dass wenn man beides findet, man umso mehr Grund zur epistemischen Freude hat. 

Aber zurück zur Studie. 
Schäfer und Schwarz (2019) haben in 9 Kategorien psychologischer Forschung jeweils zufällig 100 Effekte aus traditionell durchgeführten Studien gezogen (also 900 traditionelle Effekte) und wollten diese mit auf die gleiche Weise gezogenen Effekte aus pre-registrierten Studien vergleichen. 
Da letztere aber noch rar sind, mussten sie kurzerhand alle diese Studien, an die sie damals gelangen konnten, zum Vergleich heranziehen und konnten dann leider keine Unterteilung in die 9 Kategorien vornehmen. 
Die 9 Kategorien waren folgende: *applied*, *biological*, *clinical*, *developmental*, *educational*, *experimental*, *multidisciplinary*,  *psychoanalysis* und *social*. 
Die mathematische Psychologie mussten sie außen vor lassen, da hier kaum Effekte vorgestellt werden, sondern eher methodische Entwicklungen. 
Das bedeutsamste Ergebnis: wie erwartet, fallen die Effektstärken in der Pre-Regristrierungscharge kleiner aus. 
Bei den traditionellen Studien wurde ein Median-Effekt von *r* = 0.36 gefunden, bei den Pre-Registration Studien mit *r* = 0.16 von nur etwas weniger als der Hälfte. 

Bevor wir zu einigen der restlichen Ergebnisse kommen, müssen wir hier natürlich erst mal einhaken. 
Das Ergebnis selbst erfüllt so ziemlich die negativen Erwartungen meines inneren Descartes: Je strikter und weniger verzerrt die Studie durchgeführt wird, desto geringer die Effektstärke. 
Aber: die Effektstärke ist *nicht* deutlich kleiner als bei den angesprochenen Meta-Meta-Analysen. 
Woran liegt das? 
Die Studie von Richard, Bond und Stokes-Zoota (2003) wird von Schäfer und Schwarz tatsächlich mit dem rätselhaften Hinweis zitiert, dass die dort angegebenen Effektstärken vergleichsweise klein seien.
Meinem Gefühl entspricht das auch, denn die Effektstärken hätte ich spontan nicht als inflationär hoch eingeschätzt. 
Es scheint, dass den Autoren wirklich gut gelungen ist, ein nicht übertrieben positives Bild zu zeichnen und dass gleichzeitig die aufgenommenen Meta-Analysen dieses Bild auch gar nicht wirklich unterstützen würden. 
Die inflationär hohen Effektstärken finden sich eher in den psychologischen Fachbüchern (berichten beide Studien), die man zur Einführung im Studium durchackern muss. 
Vielleicht auch in spezifischen Subdisziplinen oder bei den Effekten, die es eben noch nicht in die zugrundegelegten Meta-Analysen geschafft haben. 
Dass Schäfer und Schwarz (2019) ihre Ergebnisse im Lichte der beiden anderen Meta-Meta-Analysen *nicht* diskutieren (und die von Richard und Kollegen nur so schwäbisch-spärlich) ist meiner Meinung nach eine Schwäche dieser Studie – das wäre wirklich spannend gewesen. 
Aber klar, man kann auch nicht alles in eine einzige Untersuchung schieben. 

Der Vollständigkeit halber: Die geringeren Effektstärken der Pre-Registrierungsstudien müssen nicht auf fragwürdige Forschungspraktiken allein zurückgeführt werden, Schäfer und Schwarz berichten z.B., dass nicht unwahrscheinlich ist, dass in diesen Studien vielleicht besonders solche Effekte untersucht werden, die eher klein oder etwas wackelig sind, bei denen man also ggf. Pech hat und anders keine Publikation erreicht. 
Forscher, die dann ihre Risiken minimieren wollen, stecken möglicherweise also einfach mehr Aufwand in die Vorarbeit einer Pre-Registrierung, was für die Forschercommunity super ist, denn dann werden auch wieder mehr Nullergebnisse unproblematisch veröffentlicht. 
Es findet vielleicht also auch eine Selbst-Selektion statt: in pre-registrierten Studien werden kleinere Effekte untersucht.

Das zweite interessante Ergebnis, dass in der Studie aufgegriffen wird, betrifft die Unterschiedlichkeit der Effektstärken, die je Subdisziplin gefunden werden (siehe Graphik unten). Hier zeigen sich deutliche Unterschiede, die es eigentlich wünschenswert erscheinen lassen, dass weitere Meta-Meta-Analysen je Subdisziplin durchgeführt werden (falls es die nicht schon gibt, das weiß ich ehrlich gesagt einfach nicht). 
Beispielsweise überschneiden sich die Konfidenzintervalle der Biopsychologen und Sozialpsychologen nicht einmal, Schäfer und Schwarz sprechen deshalb nicht zu Unrecht davon, dass diese beiden Disziplinen in unterschiedlichen Effektstärke-Universen beheimatet sind. 
Also auch ein weiteres starkes Indiz dafür, dass es nicht eine „Effectsize to rule them all“ geben sollte, sondern man nach Disziplin unterscheiden muss. 
Für die Sozialpsychologen ist das eine entspannende Nachricht, denn endlich dürfen deren klassischerweise „klein“ genannten Effekte ganz typisch genannt werden und man dürfte realistische Erwartung hegen und setzen.

![]('/images/Effektstärken_je_Subdisziplin.png')
![]('../public/images/Effektstärken_je_Subdisziplin.png')

## Zwischenfazit auf dem Weg zu "Effektstärken 3"
Zeit für ein Zwischenfazit.
Effektstärken sind nicht so trivial, wie man meinen könnte. 
Man sollte sie zunächst auf statistischer Basis verstehen und nach und nach ein Gefühl dafür entwickeln, was irgendwelche *d*s und *r*s bedeuten. 
Dann kann man den nächsten Schritt gehen: Man vergleicht Effektstärken. 
Wir können dann einschätzen, wie sichtbar ein bestimmtes *d* ist (man erinnere sich an die Körpergrößenunterschiede junger und noch jüngerer Damen), wie Effektstärken allgemein ausfallen (wie wirksam z.B. kleine blauen Pillen sind). 
Dann muss man sich einen Überblick für die entsprechenden Disziplinen schaffen. 
Wer wissen will, ob ein sozialpsychologischer Effekt groß oder klein ist, sollte den Vergleich mit Studien in Biopsychostudien nicht aufnehmen. 
Fehlen noch die zwei oben schon erwähnten Punkte: Wie sehen eigentlich Korrelationen aus? 
Und wie sehen spezifische Effekte, die wir so finden, in die Realität übersetzt aus? Beides soll im nächsten Post kommen, wenn Sie unter anderem erfahren, wie viele Leben der Aspirineffekt von *r* =  .02 wohl rettet und wie so eine Korrelation aussieht (Spoiler: man erkennt sie nicht in einem Plot).


------
--

Quellen:

Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T. H., Huber, J., Johannesson, M., ... & Altmejd, A. (2018). Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. *Nature Human Behaviour, 2*(9), 637-644.

Cohen, J. (1988). *Statistical power analysis for the behav-ioral sciences* (2nd ed.). Hillsdale, NJ: Erlbaum.   
Gibt es übrigens komplett und frei verfügbar: [hier](http://www.utstat.toronto.edu/~brunner/oldclass/378f16/readings/CohenPower.pdf)

Fraley, R. C., & Marks, M. J. (2007). The null hypothesis significance testing debate and its implications for personal-ity research. In R. W. Robins, R. C. Fraley, & R. F. Krueger (Eds.), *Handbook of research methods in personality psychology* (pp. 149–169). New York, NY: Guilford Press.

Funder, D. C., & Ozer, D. J. (2019). Evaluating effect size in psychological research: Sense and nonsense. *Advances in Methods and Practices in Psychological Science, 2*(2), 156-168.

Gignac, G. E., & Szodorai, E. T. (2016). Effect size guidelines for individual differences researchers. *Personality and individual differences, 102*, 74-78.

Meyer, G. J., Finn, S. E., Eyde, L. D., Kay, G. G., Moreland, K. L., Dies, R. R., ... & Reed, G. M. (2001). *Psychological testing and psychological assessment: A review of evidence and issues. American psychologist, 56*(2), 128.

Kleines Extra: Eine spezfische Untersuchung von Effektstärken in der Intelligenzforschung haben Nujiten und Kollegen (2020) unternommen und dabei eine mittlere Effektstärke von *r* = 0.26 gefunden.
Nuijten, M. B., van Assen, M. A., Augusteijn, H. E., Crompvoets, E. A., & Wicherts, J. M. (2020). Effect sizes, power, and biases in intelligence research: A meta-meta-analysis. *Journal of Intelligence, 8*(4), 36.

Open Science Collaboration. (2015). *Estimating the reproducibility of psychological science. Science, 349*(6251).

Richard, F. D., Bond Jr, C. F., & Stokes-Zoota, J. J. (2003). One hundred years of social psychology quantitatively described. *Review of General Psychology, 7*(4), 331-363.

Schäfer, T., & Schwarz, M. A. (2019). The meaningfulness of effect sizes in psychological research: Differences between sub-disciplines and the impact of potential biases. *Frontiers in Psychology, 10*, 813.
