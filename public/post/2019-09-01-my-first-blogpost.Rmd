---
title: My first Blogpost
author: Simon Buettner
date: '2019-09-01'
slug: my-first-blogpost
categories: []
tags: []
keywords:
  - tech
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(psych)
library(lavaan)
library(mirt)
library(dplyr)
load("../../static/data/ProtestandWorkEthic/PWES.Rdata")
```
# Beginning a Blog
In my first blog post I want to investigate a data set that I found on <https://openpsychometrics.org/_rawdata/>, namely the "Protestant Work Ethic Scale" consisting of 19 likert scale items with n = 558 answers. Ever since I was interested in predictors of job success, where in numerous studies it shown that following measures of General Intelligence, Conscientiousness (and/or Integrity) was the leading predictor of job success, I thought "Wow, so old fashioned virtues really do hold today (overall) when it comes to being succesful. Why wouldn't someone build a scale to measure just those virtues?" More explicitly, since in Germany we know the classic German virtues (orderliness, being punctual, working hard etc.) which are basically derived from Prussian virtues and as of today are still THE virtues people in Suevia where I was "born and raised" and still live today, I found some amusement in thinking of a "German/Prussian/Suevian virtues scale" (in german a "schwäbische Hausfrauenskala" would reflect this in a more humorous manner). Fast forward to last friday when I wanted to have a peek at freely available datasets that I could investigate with some models available with current R psychometric packages, I find this little gem that goes back to Max Webers "Die protestantische Ethik und der Geist des Kapitalismus" where he basically posits that the rather ascetic virtues of protestants in northern europe played a key role in the development of (early) capitalism. Naturally I was excited to investigate this data. Unfortunately, there is no possibility to investigate whether this scale predicts job success, nonetheless the underlying structure of this scale might reveal some great insights (and to be honest fitting some models is fun in and of itself). Let's do it then.

But first, what does this scale even look like? Here are the items:

* Q1	Most people spend too much time in unprofitable amusements. 
* Q2	Our society would have fewer problems if people had less leisure time. 
* Q3	Money acquired easily (e.g. through gambling or speculation) is usually spent unwisely. 
* Q4	There are few satisfactions equal to the realization that one has done one's best at a job. 
* Q5	The most difficult college courses usually turn out to be the most rewarding. 
* Q6	Most people who don’t succeed in life are just plain lazy. 
* Q7	The self-made person is likely to be more ethical than someone who is born to wealth.  
* Q8	I often feel I would be more successful if I sacrificed certain pleasures. 
* Q9	People should have more leisure time to spend in relaxation.  
* Q10	Anyone who is able and willing to work hard has a good chance of succeeding. 
* Q11	People who fail at a job have usually not tried hard enough. 
* Q12	Life would have very little meaning if we never had to suffer. 
* Q13	Hard work offers little guarantee of success. 
* Q14	The credit card is a ticket to careless spending. 
* Q15	Life would be more meaningful if we had more leisure time. 
* Q16	The person who can approach an unpleasant task with enthusiasm is the one who gets ahead. 
* Q17	If one works hard enough they are likely to make a good life for themselves. 
* Q18	I feel uneasy when there is little work for me to do. 
* Q19	A distaste for hard work usually reflects a weakness of character.

As you can see, this is scale kind of represents your (grand-)parents waving their finger and telling you not to waste your time, to save some money, and that you should work hard. Before taking a first look at the underlying data structure, I will take a away one result: Q7 and 14 kind of don't work in any factorial structure and just add some noise to the data. So, for the further analyses, I excluded them. 

This already tells us a little something: credit card spending was not really seen as being associated with the rather a rather ascetic ethic. I'd guess this item was kind of meant to assess the way people feel about taking credit in order to buy something instead of waiting until they have saved enough money to buy it (in general I believe this idea would not work too well in every country. In the U.S. this might be rather acceptable while in germany the only thing you'd buy on debt is a house - at least in my social surrounding). But the items failed to reflect this mindset, and the data shows it. 

Item Q7 kind of targets the believe that rich people are somehow decadent or at least more decadent than a self-made man (and perhaps even don't deserve their wealth). I can see why this does not really fit the rest of the scale because it targets a rather specific oppinion, that does not really need to be in line with the general ethical principles glimmering through the rest of these items.

# Investigating the Factor Structure: fa.parallel()
Enough talk, let's get down to business. To investigate the underlying data structure, the function fa.parallel() really is the way to go. It creates a Factor Analysis and a Principal Component Analysis Scree Plot. And not only that, it also simulates scree plots with randomly filled data matrices identical in structure to your data. This way, you can see how much of the structure you find is just noise and how much of it should be considered signal, i.e. nonrandom "real" structure. Since I am not really familiar with the FA Scree Plot, I usually just look at the PCA one mainly but try to use the FA plot in a rather informal way. In our case, we clearly see that there is one big component that we should extract and one additional factor less than 3 times as important. The FA Scree Plot would suggest that there are up to 6 more factors (but again way less important than the big first factor) to be considered, though I guess only 4 of them would be reasonable to have a look at. So, whatever we are going to model, we will need one big factor for sure and up to 4 smaller factors.

```{r}
fa.parallel(dat)
```

# What factors are there? omega()
Next we want to see how items actually go together. The easiest way to investigate this (imo), is to just use the plot that omega() gives us.
Again I'm kind of taking away some of the upcoming results, but overall, the extraction of a 4 factor structure will lead to some good looking und easy to use results. Thus, we will only look at the omega plot for 4 factors. Since omega will use a bifactor model, this means we will get 1 general factor and 3 narrower factors - exactly what our fa.parallel result would urge us to do. I'm not going to interpret these results in detail, because I really just use the omega plot for some preliminary data inspection. The reason for this is rather simple: the omega plot gives us a bifactor structure, but I don't really know if I want to trust this one, because it allows crossloadings and uses a classical linear metric approach to get it's solutions while later on I will prefer item response theory approaches that actually model the association between latent construct und response behaviour. In my experience, the omega bifactor structure does not converge perfectly with the in depth analysis that IRT modelling allows us to do.

```{r}
omega(dat,3)
```


# The graded response model: model fit
Being an IRT fanboy, I usually test various modelling approaches to see how to best model my data. I recently started using lavaan more often, mainly because it allows for faster model estimation and because it can propose relations to be modelled. But since what I am really interested in can only be investigated through some IRT modelling, lavaan is just my henchman for the following "real" analysis. For this analysis, I rely on the "mirt" package from R. Philip Chalmers. It is the most flexible IRT modelling package so far, and if you have some time to dive into one package, mirt is really the go to package. I startet this paragraph by outing my self as an IRT fanboy, but I'm even a bigger mirt fanboy and would probably not be the former if it wasn't for the latter. 

The first thing we will do is just fit a unidimensional model, because to be honest, the Scree Plot analysis really could just make me ignore any more structure than the one big factor. I've included the way I would estimate the mirt models, but some of the modelling just takes up too much time, so I commented these strips out.
```{r}
# gmodel1 <- mirt(dat, 1, "graded", TOL = .0001)
summary(gmodel1)
# (m2.gmodel1<-M2(gmodel1, type = "C2"))
m2.gmodel1
itemfit(gmodel1, method = "PV_Q1")
```
The summary shows us two things: The first factor explains a good chunk of the variance in the data (41%) and all items load decently on this general factor.
"M2" gives us some CFA indices and we can see, the one dimensional graded response model fits rather decently! However, I must remind you, we already excluded 2 items at this point. Additionally, there is still some room for improvements in every CFA index.
Another thing we straight up want to look at is the model conformity of single items. Here too, in terms of significance, we do not find heavily misfitting items. 

In order to see what that means, we can have a quick look at the empirical plot of the misfitting and a fitting item. And what we see is ... well not to much. There are some data points that don't really the expected pattern, but we do not see anything extreme. I think most disturbing is that the curve for Category 1 is really steep towards the lower where there are not a lot of data points.

```{r}
itemfit(gmodel1, empirical.plot = 9, group.bins = 16)
itemfit(gmodel1, empirical.plot = 16, group.bins = 16)
```


# The graded response model: person parameters
Next we want to make some analysis to go in direction of applicability of the scale. Therefor we have a look at the reliability of the 1 dimensional GRM and the theta (person parameter) distribution. 
The theta distribution is really disturbing. To be honest, the distribution very much seems to give us and our modelling approach the finger. Yikes! How would we even use this in a real life setting? Normal distribution not in sight.


```{r}
# thetgm1 <- fscores(gmodel1, full.scores.SE = TRUE, method = "EAP")

hist(thetgm1, breaks = 40)
```

Oh well, who cares really? Let's rather turn to the reliability of the scale. This time we're not disappointed: It's high throughout the expectable theta range! This does not help us in respect of middle finger peak of the theta distribution (we cannot differentiate a big part of the persons because they are so heavily clustered). 
At this point you might be wondering two things: 1. Wait, why is the reliability dependend on theta? 2. Wait, why is this guy coming back to reliability despite the IRT framework? Glad you asked! 

1. In item response theory there is no "natural" single value to reflect how precise our test measures the underlying ability. Instead, every item has the capacity to inform us about the underlying latent trait of a single person. This capacity is an item specific attribute and is called "item information". The item information is a function of the theta parameter, i.e., some items are decent in order to differentiate between persons on the higher end of the theta spectrum, some in the middle (my experience is: mostly in the middle) and some in the lower end. The easiest way to grasp this is to think of some math test. In this test you ask participants "2+2= ?". Clearly, this item won't help us differentiate between anyone beyond middleschool because every one will get it right. However, to test low ability persons (kindergarden kids that just started learning about calculating) this item might just be perfect. Asking for the solution of some complex equation containing some weird integrals however, might help differentiate bteween test persons after middleschool, but asking some kindergarten kids to give you an answer  won't help differentiation between these kids (no matter how many sweets you promise for a correct answer). 
So much about item information. The nice thing in IRT is that in order to know how precise a test is overall, you just have to add the single iteminformation functions. The result is the test information function that is depicted in terms of the  "reliability" as used in classical test theory.

2. Since only a few "normal" psychologists know a big deal about IRT, the use of "reliability" makes things a lot more comprehensible. Of course, this reliability depends perfectly on the test information (and thus on the Standard Error of the theta).


```{r}
empirical_rxx(thetgm1)
plot(gmodel1, type = "rxx", theta_lim = c(-2,2) )
```

Since we get the fat clustering of theta values, we're gonna be interested in the model more in general. We will therefor look at the expected score given our model, at the expected chosen category of the model and the expected item score - all depending on theta. We do not really find oddities here. Except that (plot 2) item response behavior is not in line with the usual likert scale assumption of equal intervals between item scores.
```{r}
plot(gmodel1, type = "score", theta_lim = c(-2,2))
plot(gmodel1, type = "trace", theta_lim = c(-2,2))
plot(gmodel1, type = "itemscore", theta_lim = c(-2,2))
```

# The graded response model: short summary
So far we're kind of happy, because our model- and itemfit is decent and we get a great reliability. Yay! However, the theta we get is, imho, just trash. What to do about that? We need some different modelling approach, and, lucky as we are, a different modelling approach can be used righteously - just remember our extra small factors!


# Bifactor models of protestant ethics.

## 2 Specific factors

```{r}
spec1 <- rep(NA, 18)
spec1[c(10,16,13)] <- 1
spec1[c(9,14,2)] <- 2
# bf1 <- bfactor(dat, spec1, TOL = .0001)
summary(bf1)
```

```{r}
theta_bf1 <- fscores(bf1, full.scores.SE = TRUE)
hist(theta_bf1[,"F1"])
hist(theta_bf1[,"F2"])
hist(theta_bf1[,"F3"])
empirical_rxx(theta_bf1)
```


## 3 Specific factors

```{r}
spec2 <- rep(NA, 18)
spec2[c(6,4,17)] <- 1
spec2[c(10,16,13)] <- 2
spec2[c(9,14,2)] <- 3
# bf2 <- bfactor(dat, spec2, TOL = .001)
summary(bf2)
```


```{r}
theta_bf2 <- fscores(bf2, full.scores.SE = TRUE)
hist(theta_bf2[,"F1"], breaks = 18)
hist(theta_bf2[,"F2"])
hist(theta_bf2[,"F3"])
empirical_rxx(theta_bf2)
```

